{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Big-Picture Surveys & Position Papers (Deployment / Serving Focus)**\n",
        "\n",
        "**Machine Learning Systems: A Survey from a Data-Oriented Perspective – Cabrera et al., ACM Computing Surveys, 2025.**  \n",
        "Broad survey of ML systems (data, training, serving, monitoring) with explicit guidance on deployment and real-world constraints.\n",
        "\n",
        "**A Survey of Large-Scale Deep Learning Serving System Optimization: Challenges and Opportunities – Yu et al., 2021.**  \n",
        "First dedicated survey on large-scale DL serving (LDS), including latency, cost, hardware heterogeneity, batching, and scheduling.\n",
        "\n",
        "**Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems – Miao et al., ACM Computing Surveys, 2025.**  \n",
        "End-to-end survey of LLM serving: KV-cache tricks, quantization, scheduling, distributed inference, and system design.\n",
        "\n",
        "**Machine Learning Inference Serving Models in Serverless Computing: A Survey – Aslani & Ghobaei-Arani, Computing, 2025.**  \n",
        "Taxonomy of serverless ML inference: SLO-aware, acceleration-aware, framework-aware, and latency-aware approaches.\n",
        "\n",
        "**Tiny Machine Learning and On-Device Inference: A Survey of Applications, Challenges, and Future Directions – Heydari & Mahmoud, Sensors, 2025.**  \n",
        "Survey of TinyML deployment on microcontrollers/edge, including latency, energy, and memory constraints.\n",
        "\n",
        "**Desiderata for Next Generation of ML Model Serving – Akoush et al., NeurIPS DMML Workshop, 2022.**  \n",
        "Position paper listing design goals (data-centricity, observability, composable inference graphs, multi-tenancy, etc.) for future serving platforms.\n",
        "\n",
        "---\n",
        "\n",
        "# **2. Production ML Platforms & End-to-End Deployment**\n",
        "\n",
        "**TFX: A TensorFlow-Based Production-Scale Machine Learning Platform – Baylor et al., KDD 2017.**  \n",
        "Google’s production ML platform: data validation, training, validation, and serving at scale.\n",
        "\n",
        "**TensorFlow-Serving: Flexible, High-Performance ML Serving – Olston et al., NeurIPS ML Systems Workshop, 2017.**  \n",
        "Describes TF-Serving; covers lifecycle, model versioning, and performance-optimized inference paths.\n",
        "\n",
        "**Training and Serving Machine Learning Models at Scale – Various Authors, 2022–2023.**  \n",
        "Discusses joint management of training and serving infrastructure, SLA constraints, and automation of ML “services” lifecycle.\n",
        "\n",
        "**Serving Deep Learning Models with Deduplication from Relational Databases – Zhou et al., VLDB 2022.**  \n",
        "Co-design of data storage and model serving; reduces latency from decoupled systems and mitigates memory pressure for large models.\n",
        "\n",
        "**Serving Deep Learning Models from Relational Databases – Follow-up Work, 2024.**  \n",
        "Extends RDBMS-integrated inference with caching and indexing strategies tuned for serving workloads.\n",
        "\n",
        "---\n",
        "\n",
        "# **3. General Model-Serving Systems (Classical → LLM)**\n",
        "\n",
        "**Clipper: A Low-Latency Online Prediction Serving System – Crankshaw et al., NSDI 2017.**  \n",
        "General-purpose prediction serving: caching, batching, adaptivity, and model selection to balance latency and throughput.\n",
        "\n",
        "**AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving – Li et al., OSDI 2023.**  \n",
        "Uses model parallelism to multiplex heterogeneous model workloads and control burstiness–latency trade-offs.\n",
        "\n",
        "**ORCA: A Distributed Serving System for Transformer-Based Generative Models – Yu et al., OSDI 2022.**  \n",
        "GPT-3 scale serving with iteration-level scheduling and parallelism strategies that outperform FasterTransformer.\n",
        "\n",
        "**Efficient Memory Management for Large Language Model Serving with PagedAttention (vLLM) – Kwon et al., 2023/2024.**  \n",
        "Introduces PagedAttention and vLLM, significantly reducing KV-cache fragmentation and boosting throughput.\n",
        "\n",
        "**FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU – Sheng et al., 2023.**  \n",
        "Offloading-based system enabling LLM serving under extreme GPU memory limitations.\n",
        "\n",
        "**Punica: Multi-Tenant LoRA Serving – Chen et al., MLSys 2024.**  \n",
        "Specialized kernel + scheduler enabling simultaneous serving of many LoRA adapters over a single base model.\n",
        "\n",
        "**Serving Deep Learning Models with Deduplication from RDBMS – Zhou et al., VLDB 2022.**  \n",
        "Also belongs here as a hybrid model-serving and data-serving system tightly integrated with relational databases.\n",
        "\n",
        "---\n",
        "\n",
        "# **4. Hardware / GPU-Level Optimizations for Inference at Scale**\n",
        "\n",
        "**Serving DNNs in Real Time at Datacenter Scale with Project Brainwave – Chung et al., IEEE Micro, 2018.**  \n",
        "Microsoft’s FPGA-based DNN serving platform for ultra-low-latency cloud inference.\n",
        "\n",
        "**A Configurable Cloud-Scale DNN Processor for Real-Time AI – Papamichael et al., ISCA 2018.**  \n",
        "Custom DNN accelerator used in Brainwave; discusses hardware–software co-design.\n",
        "\n",
        "**Dynamic Space-Time Scheduling for GPU Inference – Jain et al., NeurIPS ML Systems Workshop, 2018.**  \n",
        "Explores space/time multiplexing strategies to share GPUs across many latency-critical inference workloads.\n",
        "\n",
        "---\n",
        "\n",
        "# **5. Edge, TinyML, and On-Device Deployment**\n",
        "\n",
        "**Tiny Machine Learning and On-Device Inference: A Survey of Applications, Challenges, and Future Directions – Heydari & Mahmoud, Sensors 2025.**  \n",
        "Covers compression techniques, low-power hardware platforms, and deployment methodology for microcontrollers and edge SoCs.\n",
        "\n",
        "---\n",
        "\n",
        "# **6. Comparative Evaluations & Tool-Focused Studies**\n",
        "\n",
        "**Evaluation of Tools and Frameworks for Machine Learning Model Serving – Beck et al., ICSE SEIP 2025.**  \n",
        "Benchmarks ML serving frameworks (TF-Serving, KFServing, TorchServe, etc.) on NLP workloads; compares scalability, usability, and performance.\n",
        "\n",
        "**Serving Deep Learning Models from Relational Databases / Serving Deep Learning Models with Deduplication from RDBMS – Zhou et al., VLDB 2022.**  \n",
        "Doubles as an evaluation of RDBMS-centric serving vs standalone model-serving systems.\n",
        "\n",
        "---\n",
        "\n",
        "# **7. “Meta” / Architecture & Design Discussions**\n",
        "\n",
        "**Machine Learning Systems: A Survey from a Data-Oriented Perspective – Cabrera et al., 2025.**  \n",
        "Includes a dedicated section covering ML deployment architectures, monitoring, and data-centric systems.\n",
        "\n",
        "**Desiderata for Next Generation of ML Model Serving – Akoush et al., 2022.**  \n",
        "Conceptual design principles and architectural requirements for modern serving stacks.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "scM3WEX1aFmO"
      }
    }
  ]
}